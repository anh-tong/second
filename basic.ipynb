{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecfa63b96582461c8bb09ef7882650c11fd38af380cc5e83709fdd5945a664c6",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Some basic functions of auto differentiation\n",
    "\n",
    "Start with package import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.autograd import grad\n",
    "from second import jvp, hvp"
   ]
  },
  {
   "source": [
    "Create a simple model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 5), \n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(5, output_dim))\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "source": [
    "Create a dummy data and compute the loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, input_dim)\n",
    "y = torch.randn(1,1)\n",
    "output = model(x)\n",
    "loss = criterion(output, y)"
   ]
  },
  {
   "source": [
    "Perform autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-0.0283, -0.0011,  0.0244],\n",
       "         [-0.1512, -0.0061,  0.1303],\n",
       "         [-0.0452, -0.0018,  0.0390],\n",
       "         [-0.1673, -0.0067,  0.1442],\n",
       "         [ 0.2585,  0.0104, -0.2228]]),\n",
       " tensor([-0.0269, -0.1434, -0.0429, -0.1588,  0.2453]),\n",
       " tensor([[0.8887, 1.0663, 0.5952, 1.3538, 1.2464]]),\n",
       " tensor([2.6375]))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "gradient = grad(\n",
    "    loss,\n",
    "    tuple(model.parameters())\n",
    ")\n",
    "gradient"
   ]
  },
  {
   "source": [
    "The above result of gradients contains the weights and biases for two layers (4 tensors in total)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Jacobian-vector product (JVP)\n",
    "Given a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian matrix $\\nabla f$ is an $m \\times n$ matrix. The Jacobian-vector product $(\\nabla f) \\mathbf{v}$ is the product of an $m \\times n$ matrix and an $n \\times 1$ vector. In the ```second.py```, we implement this function as\n",
    "```\n",
    "def jvp(outputs, inputs, vector, create_graph=False):\n",
    "    \"\"\"Jacobian vector product\n",
    "        This version where vector is flatten\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(outputs, tuple):\n",
    "        dummy = [torch.zeros_like(o, requires_grad=True) for o in outputs]\n",
    "    else:\n",
    "        dummy = torch.zeros_like(outputs, requires_grad=True)\n",
    "\n",
    "    jacobian = grad(outputs,inputs, grad_outputs=dummy, create_graph=True)\n",
    "    Jv = grad(parameters_to_vector(jacobian), dummy, grad_outputs=vector, create_graph=create_graph)\n",
    "    return parameters_to_vector(Jv)\n",
    "```\n",
    "\n",
    "We need this implementation in Pytorch because the existing ```torch.autograd.grad``` only allow the option vector-Jacobian product which is $\\mathbf{v}^\\top\\nabla f$, a product of an $m \\times 1$ vector and $m \\times n$ matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = []\n",
    "for p in model.parameters():\n",
    "    vector += [torch.randn_like(p)]\n",
    "\n",
    "Jv = jvp(loss, tuple(model.parameters()), vector=parameters_to_vector(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jv2 = grad(loss, tuple(model.parameters()), grad_outputs=torch.zeros_like(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anhth/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "Hv = hvp(loss, tuple(model.parameters()), vector=parameters_to_vector(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 1.0711,  0.3990, -0.1729, -0.4672, -0.1740,  0.0754, -0.9558, -0.3560,\n",
       "         0.1543, -0.9900, -0.3688,  0.1598, -1.3511, -0.5032,  0.2181,  1.0908,\n",
       "        -0.4758, -0.9734, -1.0082, -1.3759, -0.6078, -1.9473, -2.4624, -4.1942,\n",
       "        -4.7859, -6.3056])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "Hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}