{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecfa63b96582461c8bb09ef7882650c11fd38af380cc5e83709fdd5945a664c6",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Advanced topics\n",
    "The following will dedicate for some second-order (Newton) optimization methods or related topics which require the second order derivative information.\n",
    "\n",
    "We define some mathematical notations including $\\mathbf{H}$ for Hessian matrix, $\\mathbf{G}$ for Gauss-Newton Hessain matrix. \n",
    "\n",
    "Suppose we are trying to optimize\n",
    "$$\\min_x f(x)$$\n",
    "At a step $k+1$ of a gradient descent, the update will be done as\n",
    "$$x^{(k+1)} = x^{(k)} - \\eta \\times \\nabla f(x^{(k)})$$\n",
    "This is called first-order optimization since we only consider first-order derivative information. Second-order optimization, on the other hand, does not require to set learning rate $\\eta$ but precomputed based on the second-order derivative. One can use the following update\n",
    "$$x^{(k+1)} = x^{(k)} - \\mathbf{H}^{-1} \\nabla f(x^{(k)})$$\n",
    "or\n",
    "$$x^{(k+1)} = x^{(k)} - \\mathbf{G}^{-1} \\nabla f(x^{(k)})$$\n",
    "\n",
    "Now, the computational challenge would lie in how to efficiently obtain the inverse of Hessian which is often expensive to compute. One of the common resorts is to use iterative algorithms. The approaches includes Conjugate Gradient (CG), Neumann series approximation and Knonecker Factorization Approximation Curvature (KFAC).\n",
    "\n",
    "**Conjugate Gradient** Instead of finding $\\mathbf{H}^{-1} \\nabla f(x^{(k)})$, we're going to solve an optimization problem which will result in a same value:\n",
    "$$\\min_a a^\\top\\mathbf{H}a + (\\nabla f)^\\top a$$\n",
    "\n",
    "Finding inverse Hessian using such methods can be seen at some work such as \n",
    "\n",
    "1. [Meta-Learning with Implicit Gradients](https://arxiv.org/abs/1909.04630)\n",
    "\n",
    "However, one important assumption that Conjugate Gradient makes is that $\\mathbf{H}$ or $\\mathbf{G}$ is possitive definite. This is not always true for Hessian $\\mathbf{H}$. \n",
    "\n",
    "**Neumann series**\n",
    "With a certain assumption (maximum spectral value less than 1), we can have the following series expansion of an inverse matrix\n",
    "$$A^{-1} = \\sum_{i=0}^\\infty (I - A)^i$$\n",
    "\n",
    "1. [Second-Order Stochastic Optimization for Machine Learning in Linear Time](https://arxiv.org/abs/1602.03943)\n",
    "\n",
    "2. [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)\n",
    "\n",
    "3. [Optimizing Millions of Hyperparmeters by Implicit Differentiation](https://arxiv.org/abs/1911.02590)\n",
    "\n",
    "**KFAC** Inspired by graphical approximation, Fisher information is estimated with structure breakdown in neural works where layers become independent in the approximation. This relaxation is convenient for inversion operation and Knonecker product becomes handy. The reference consists of\n",
    "1. [Optimizing Neural Networks with Kronecker-factored Approximate Curvature](https://arxiv.org/abs/1503.05671)\n",
    "\n",
    "The following notebook tries to cover such methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "from second import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(mvp, vector, parameters, cg_iter=10, eps=0.):\n",
    "    \"\"\"\n",
    "    Conjugate gradient method\n",
    "    Reference: https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "    Arguments:\n",
    "        mvp: matrix vector product as a callable function. In this case, it corresponds to Hessian-vector product\n",
    "        vector: a vector. In this case, it is the gradient vector\n",
    "        parameters: parameters\n",
    "    \"\"\"\n",
    "    x = vector.clone().detach()\n",
    "    # residual\n",
    "    r = vector.clone().detach() - mvp(x)\n",
    "    p = r.clone().detach()\n",
    "    for _ in range(cg_iter):\n",
    "        Ap = mvp(p)\n",
    "        alpha = (r @ r) / (p @ Ap + eps)\n",
    "        x = x + alpha * p\n",
    "        r_new = r - alpha * Ap\n",
    "        beta = (r_new @ r_new) / (r @ r + eps)\n",
    "        p = r_new + beta * p\n",
    "        r = r_new.clone().detach()\n",
    "    return x"
   ]
  },
  {
   "source": [
    "Create a simple model and compute a loss value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "output_dim = 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 5),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(5, output_dim)\n",
    ")\n",
    "\n",
    "x = torch.randn(10, input_dim)\n",
    "y = torch.randn(10, output_dim)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "source": [
    "Define the MVP as the Hessian-vector product as one input of ```conjugate_gradient``` method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_mvp(vector):\n",
    "    return hvp(loss, tuple(model.parameters()), vector)\n",
    "    # return gvp(loss, logit, tuple(model.parameters()), vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = model(x)\n",
    "loss = criterion(logit, y)\n",
    "gradient = grad(loss,tuple(model.parameters()), retain_graph=True)\n",
    "cg_inv_hvp = conjugate_gradient(cg_mvp, vector=parameters_to_vector(gradient), parameters=tuple(model.parameters()), cg_iter=40)"
   ]
  },
  {
   "source": [
    "Now, let's see what the exact computation looks like (of course the following should be avoided in practice)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_hessian(loss, parameters):\n",
    "    gradient = grad(\n",
    "        loss,\n",
    "        parameters,\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )\n",
    "    gradient = parameters_to_vector(gradient)\n",
    "    hessian = torch.zeros((gradient.numel(), gradient.numel()))\n",
    "    for i in range(gradient.numel()):\n",
    "        H_i = grad(\n",
    "            gradient[i],\n",
    "            parameters,\n",
    "            retain_graph=True\n",
    "        )\n",
    "        H_i = parameters_to_vector(H_i)\n",
    "        hessian[i] = H_i\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = model(x)\n",
    "loss = criterion(logit, y)\n",
    "hessian = exact_hessian(loss, tuple(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_inv_hvp = torch.inverse(hessian) @ parameters_to_vector(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(4.3236)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "values, _= torch.eig(hessian)\n",
    "torch.max(values)"
   ]
  },
  {
   "source": [
    "Compute the error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6924)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "relative_error = torch.norm(cg_inv_hvp - exact_inv_hvp) / torch.norm(exact_inv_hvp)\n",
    "relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7375)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cosine_similarity = cg_inv_hvp @ exact_inv_hvp / (torch.norm(cg_inv_hvp) * torch.norm(exact_inv_hvp))\n",
    "cosine_similarity"
   ]
  },
  {
   "source": [
    "We can see that the approximation method (CG here) return a good estimate based on the cosine similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neumann(mvp, parameters, vector, truncate_iter=10):\n",
    "    p = v = vector\n",
    "    for _ in range(truncate_iter):\n",
    "        p = v + p - mvp(p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_inv_hvp= neumann(cg_mvp, tuple(model.parameters()), vector=parameters_to_vector(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0752,  0.0680, -0.3132, -0.2627,  0.3481,  0.3965, -0.0778,  0.1506,\n",
       "         0.1076, -0.1436,  0.1586,  0.2219, -0.2498,  0.2461,  0.4386, -0.3126,\n",
       "        -0.2587, -0.1872, -0.1226, -0.0814, -1.1904, -0.5912, -0.3178, -0.1558,\n",
       "         0.0867, -0.9277])"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "def neuman_this_case(parameters, vector, truncate_iter=10):\n",
    "    v = vector\n",
    "    p = v.clone().detach()\n",
    "    for i in range(truncate_iter):\n",
    "        x_i = x[i]\n",
    "        y_i = y[i]\n",
    "        output_i = model(x_i)\n",
    "        loss_i = criterion(output_i, y_i)\n",
    "        p = v + p - hvp(loss, parameters, p)/4.5\n",
    "    \n",
    "    return p\n",
    "\n",
    "nm_inv_hvp = neuman_this_case(tuple(model.parameters()), vector=parameters_to_vector(gradient))\n",
    "nm_inv_hvp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = nm_inv_hvp @ exact_inv_hvp / (torch.norm(nm_inv_hvp) * torch.norm(exact_inv_hvp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.0067)"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}